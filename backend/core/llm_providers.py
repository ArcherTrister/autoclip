"""
å¤šæ¨¡å‹æä¾›å•†ç»Ÿä¸€æ¥å£
æ”¯æŒOpenAIã€Geminiã€ç¡…åŸºæµåŠ¨ã€é˜¿é‡ŒDashScopeç­‰
"""
import json
import logging
import os
from abc import ABC, abstractmethod
from typing import Dict, Any, List, Optional, Union
from enum import Enum
from dataclasses import dataclass
from pathlib import Path

logger = logging.getLogger(__name__)

class ProviderType(Enum):
    """æ¨¡å‹æä¾›å•†ç±»å‹"""
    DASHSCOPE = "dashscope"  # é˜¿é‡Œé€šä¹‰åƒé—®
    OPENAI = "openai"        # OpenAI
    GEMINI = "gemini"        # Google Gemini
    SILICONFLOW = "siliconflow"  # ç¡…åŸºæµåŠ¨

@dataclass
class ModelInfo:
    """æ¨¡å‹ä¿¡æ¯"""
    name: str
    display_name: str
    provider: ProviderType
    max_tokens: int
    cost_per_token: Optional[float] = None
    description: Optional[str] = None

@dataclass
class LLMResponse:
    """LLMå“åº”"""
    content: str
    usage: Optional[Dict[str, Any]] = None
    model: Optional[str] = None
    finish_reason: Optional[str] = None

class LLMProvider(ABC):
    """LLMæä¾›å•†æŠ½è±¡åŸºç±»"""
    
    def __init__(self, api_key: str, model_name: str, **kwargs):
        self.api_key = api_key
        self.model_name = model_name
        self.kwargs = kwargs
    
    @abstractmethod
    def call(self, prompt: str, input_data: Any = None, **kwargs) -> LLMResponse:
        """
        è°ƒç”¨æ¨¡å‹API
        
        Args:
            prompt: æç¤ºè¯
            input_data: è¾“å…¥æ•°æ®
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            LLMResponse: æ¨¡å‹å“åº”
        """
        pass
    
    @abstractmethod
    def test_connection(self) -> bool:
        """
        æµ‹è¯•APIè¿æ¥
        
        Returns:
            bool: è¿æ¥æ˜¯å¦æˆåŠŸ
        """
        pass
    
    @abstractmethod
    def get_available_models(self) -> List[ModelInfo]:
        """
        è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
        
        Returns:
            List[ModelInfo]: å¯ç”¨æ¨¡å‹åˆ—è¡¨
        """
        pass
    
    def _build_full_input(self, prompt: str, input_data: Any = None) -> str:
        """æ„å»ºå®Œæ•´çš„è¾“å…¥"""
        if input_data:
            if isinstance(input_data, dict):
                return f"{prompt}\n\nè¾“å…¥å†…å®¹ï¼š\n{json.dumps(input_data, ensure_ascii=False, indent=2)}"
            else:
                return f"{prompt}\n\nè¾“å…¥å†…å®¹ï¼š\n{input_data}"
        return prompt

class DashScopeProvider(LLMProvider):
    """é˜¿é‡ŒDashScopeæä¾›å•†"""
    
    def __init__(self, api_key: str, model_name: str = "qwen-plus", **kwargs):
        super().__init__(api_key, model_name, **kwargs)
        try:
            from dashscope import Generation
            self.generation = Generation
        except ImportError:
            raise ImportError("è¯·å®‰è£…dashscope: pip install dashscope")
    
    def call(self, prompt: str, input_data: Any = None, **kwargs) -> LLMResponse:
        """è°ƒç”¨DashScope API"""
        try:
            full_input = self._build_full_input(prompt, input_data)
            
            response_or_gen = self.generation.call(
                model=self.model_name,
                prompt=full_input,
                api_key=self.api_key,
                stream=False,
                **kwargs
            )
            
            # å¤„ç†å“åº”
            # DashScopeçš„GenerationResponseè™½ç„¶æœ‰__iter__æ–¹æ³•ï¼Œä½†ä¸æ˜¯çœŸæ­£çš„è¿­ä»£å™¨
            # ç›´æ¥ä½¿ç”¨å“åº”å¯¹è±¡æœ¬èº«
            response = response_or_gen
            
            if response and response.status_code == 200:
                if response.output and response.output.text is not None:
                    return LLMResponse(
                        content=response.output.text,
                        model=self.model_name,
                        finish_reason=getattr(response.output, 'finish_reason', None)
                    )
                else:
                    finish_reason = getattr(response.output, 'finish_reason', 'unknown') if response.output else 'unknown'
                    logger.warning(f"APIè¯·æ±‚æˆåŠŸï¼Œä½†è¾“å‡ºä¸ºç©ºã€‚ç»“æŸåŸå› : {finish_reason}")
                    return LLMResponse(content="")
            else:
                code = getattr(response, 'code', 'N/A')
                message = getattr(response, 'message', 'æœªçŸ¥APIé”™è¯¯')
                raise Exception(f"APIè°ƒç”¨å¤±è´¥ - Status: {response.status_code}, Code: {code}, Message: {message}")
                
        except Exception as e:
            logger.error(f"DashScopeè°ƒç”¨å¤±è´¥: {str(e)}")
            raise
    
    def test_connection(self) -> bool:
        """æµ‹è¯•DashScopeè¿æ¥"""
        try:
            response = self.call("è¯·å›å¤'æµ‹è¯•æˆåŠŸ'")
            return "æµ‹è¯•æˆåŠŸ" in response.content or "success" in response.content.lower()
        except Exception as e:
            logger.error(f"DashScopeè¿æ¥æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    def get_available_models(self) -> List[ModelInfo]:
        """è·å–DashScopeå¯ç”¨æ¨¡å‹"""
        return [
            ModelInfo(
                name="qwen-plus",
                display_name="é€šä¹‰åƒé—®Plus",
                provider=ProviderType.DASHSCOPE,
                max_tokens=8192,
                description="é˜¿é‡Œäº‘é€šä¹‰åƒé—®Plusæ¨¡å‹"
            ),
            ModelInfo(
                name="qwen-max",
                display_name="é€šä¹‰åƒé—®Max",
                provider=ProviderType.DASHSCOPE,
                max_tokens=8192,
                description="é˜¿é‡Œäº‘é€šä¹‰åƒé—®Maxæ¨¡å‹"
            ),
            ModelInfo(
                name="qwen-turbo",
                display_name="é€šä¹‰åƒé—®Turbo",
                provider=ProviderType.DASHSCOPE,
                max_tokens=8192,
                description="é˜¿é‡Œäº‘é€šä¹‰åƒé—®Turboæ¨¡å‹"
            )
        ]

class OpenAIProvider(LLMProvider):
    """OpenAIæä¾›å•†"""
    
    def __init__(self, api_key: str, model_name: str = "gpt-3.5-turbo", **kwargs):
        super().__init__(api_key, model_name, **kwargs)
        try:
            import openai
            self.client = openai.OpenAI(api_key=api_key)
        except ImportError:
            raise ImportError("è¯·å®‰è£…openai: pip install openai")
    
    def call(self, prompt: str, input_data: Any = None, **kwargs) -> LLMResponse:
        """è°ƒç”¨OpenAI API"""
        try:
            full_input = self._build_full_input(prompt, input_data)
            
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[{"role": "user", "content": full_input}],
                **kwargs
            )
            
            content = response.choices[0].message.content
            usage = {
                "prompt_tokens": response.usage.prompt_tokens,
                "completion_tokens": response.usage.completion_tokens,
                "total_tokens": response.usage.total_tokens
            } if response.usage else None
            
            return LLMResponse(
                content=content,
                usage=usage,
                model=self.model_name,
                finish_reason=response.choices[0].finish_reason
            )
            
        except Exception as e:
            logger.error(f"OpenAIè°ƒç”¨å¤±è´¥: {str(e)}")
            raise
    
    def test_connection(self) -> bool:
        """æµ‹è¯•OpenAIè¿æ¥"""
        try:
            response = self.call("è¯·å›å¤'æµ‹è¯•æˆåŠŸ'")
            return "æµ‹è¯•æˆåŠŸ" in response.content or "success" in response.content.lower()
        except Exception as e:
            logger.error(f"OpenAIè¿æ¥æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    def get_available_models(self) -> List[ModelInfo]:
        """è·å–OpenAIå¯ç”¨æ¨¡å‹"""
        return [
            ModelInfo(
                name="gpt-3.5-turbo",
                display_name="GPT-3.5 Turbo",
                provider=ProviderType.OPENAI,
                max_tokens=4096,
                description="OpenAI GPT-3.5 Turboæ¨¡å‹"
            ),
            ModelInfo(
                name="gpt-4",
                display_name="GPT-4",
                provider=ProviderType.OPENAI,
                max_tokens=8192,
                description="OpenAI GPT-4æ¨¡å‹"
            ),
            ModelInfo(
                name="gpt-4-turbo",
                display_name="GPT-4 Turbo",
                provider=ProviderType.OPENAI,
                max_tokens=128000,
                description="OpenAI GPT-4 Turboæ¨¡å‹"
            )
        ]

class GeminiProvider(LLMProvider):
    """Google Geminiæä¾›å•†"""
    
    def __init__(self, api_key: str, model_name: str = "gemini-2.5-flash", **kwargs):
        super().__init__(api_key, model_name, **kwargs)
        try:
            import google.generativeai as genai
            genai.configure(api_key=api_key)
            self.model = genai.GenerativeModel(model_name)
        except ImportError:
            raise ImportError("è¯·å®‰è£…google-generativeai: pip install google-generativeai")
    
    def call(self, prompt: str, input_data: Any = None, **kwargs) -> LLMResponse:
        """è°ƒç”¨Gemini API"""
        try:
            full_input = self._build_full_input(prompt, input_data)
            
            response = self.model.generate_content(full_input, **kwargs)
            
            return LLMResponse(
                content=response.text,
                model=self.model_name,
                finish_reason=getattr(response, 'finish_reason', None)
            )
            
        except Exception as e:
            logger.error(f"Geminiè°ƒç”¨å¤±è´¥: {str(e)}")
            raise
    
    def test_connection(self) -> bool:
        """æµ‹è¯•Geminiè¿æ¥"""
        try:
            response = self.call("è¯·å›å¤'æµ‹è¯•æˆåŠŸ'")
            return "æµ‹è¯•æˆåŠŸ" in response.content or "success" in response.content.lower()
        except Exception as e:
            logger.error(f"Geminiè¿æ¥æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    def get_available_models(self) -> List[ModelInfo]:
        """è·å–Geminiå¯ç”¨æ¨¡å‹"""
        return [
            ModelInfo(
                name="gemini-2.5-flash",
                display_name="Gemini 2.5 Flash",
                provider=ProviderType.GEMINI,
                max_tokens=1000000,
                description="Google Gemini 2.5 Flashæ¨¡å‹"
            ),
            ModelInfo(
                name="gemini-1.5-pro",
                display_name="Gemini 1.5 Pro",
                provider=ProviderType.GEMINI,
                max_tokens=2000000,
                description="Google Gemini 1.5 Proæ¨¡å‹"
            ),
            ModelInfo(
                name="gemini-1.5-flash",
                display_name="Gemini 1.5 Flash",
                provider=ProviderType.GEMINI,
                max_tokens=1000000,
                description="Google Gemini 1.5 Flashæ¨¡å‹"
            )
        ]

class SiliconFlowProvider(LLMProvider):
    """ç¡…åŸºæµåŠ¨æä¾›å•†"""
    
    def __init__(self, api_key: str, model_name: str = "Qwen/Qwen2.5-7B-Instruct", **kwargs):
        super().__init__(api_key, model_name, **kwargs)
        self.base_url = "https://api.siliconflow.cn/v1"
    
    def call(self, prompt: str, input_data: Any = None, **kwargs) -> LLMResponse:
        """è°ƒç”¨ç¡…åŸºæµåŠ¨API"""
        try:
            import requests
            
            full_input = self._build_full_input(prompt, input_data)
            
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            
            data = {
                "model": self.model_name,
                "messages": [{"role": "user", "content": full_input}],
                "stream": False,
                **kwargs
            }

            # ã€æ–°å¢ã€‘æ‰“å°è°ƒè¯•ä¿¡æ¯
            logger.info("-" * 30)
            logger.info(f"ğŸš€ æ­£åœ¨è¯·æ±‚æ¨¡å‹: {data['model']}")
            logger.info(f"ğŸ“ å‘é€çš„ Messages: {json.dumps(data['messages'], ensure_ascii=False)}")
            logger.info(f"âš™ï¸ å…¶ä»–å‚æ•°: { {k:v for k,v in data.items() if k != 'messages'} }")
            logger.info("-" * 30)

            req_timeout = kwargs.pop('timeout', (10, 120))
            
            response = requests.post(
                f"{self.base_url}/chat/completions",
                headers=headers,
                json=data,
                timeout=req_timeout
            )
            
            response.raise_for_status()
            result = response.json()
            
            content = result["choices"][0]["message"]["content"]
            usage = result.get("usage")
            
            return LLMResponse(
                content=content,
                usage=usage,
                model=self.model_name,
                finish_reason=result["choices"][0].get("finish_reason")
            )
            
        except Exception as e:
            logger.error(f"ç¡…åŸºæµåŠ¨è°ƒç”¨å¤±è´¥: {str(e)}")
            raise
    
    def test_connection(self) -> bool:
        """æµ‹è¯•ç¡…åŸºæµåŠ¨è¿æ¥"""
        try:
            response = self.call("è¯·å›å¤'æµ‹è¯•æˆåŠŸ'")
            return "æµ‹è¯•æˆåŠŸ" in response.content or "success" in response.content.lower()
        except Exception as e:
            logger.error(f"ç¡…åŸºæµåŠ¨è¿æ¥æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    def get_available_models(self) -> List[ModelInfo]:
        """è·å–ç¡…åŸºæµåŠ¨å¯ç”¨æ¨¡å‹"""
        return [
            ModelInfo(
                name="Qwen/Qwen2.5-7B-Instruct",
                display_name="Qwen2.5-7B",
                provider=ProviderType.SILICONFLOW,
                max_tokens=32768,
                description="ç¡…åŸºæµåŠ¨Qwen2.5-7Bæ¨¡å‹"
            ),
            ModelInfo(
                name="Qwen/Qwen2.5-14B-Instruct",
                display_name="Qwen2.5-14B",
                provider=ProviderType.SILICONFLOW,
                max_tokens=32768,
                description="ç¡…åŸºæµåŠ¨Qwen2.5-14Bæ¨¡å‹"
            ),
            ModelInfo(
                name="Qwen/Qwen2.5-32B-Instruct",
                display_name="Qwen2.5-32B",
                provider=ProviderType.SILICONFLOW,
                max_tokens=32768,
                description="ç¡…åŸºæµåŠ¨Qwen2.5-32Bæ¨¡å‹"
            ),
            ModelInfo(
                name="deepseek-ai/DeepSeek-V2.5",
                display_name="DeepSeek-V2.5",
                provider=ProviderType.SILICONFLOW,
                max_tokens=65536,
                description="ç¡…åŸºæµåŠ¨DeepSeek-V2.5æ¨¡å‹"
            )
        ]

class LLMProviderFactory:
    """LLMæä¾›å•†å·¥å‚"""
    
    _providers = {
        ProviderType.DASHSCOPE: DashScopeProvider,
        ProviderType.OPENAI: OpenAIProvider,
        ProviderType.GEMINI: GeminiProvider,
        ProviderType.SILICONFLOW: SiliconFlowProvider,
    }
    
    @classmethod
    def create_provider(cls, provider_type: ProviderType, api_key: str, model_name: str, **kwargs) -> LLMProvider:
        """åˆ›å»ºæä¾›å•†å®ä¾‹"""
        if provider_type not in cls._providers:
            raise ValueError(f"ä¸æ”¯æŒçš„æä¾›å•†ç±»å‹: {provider_type}")
        
        provider_class = cls._providers[provider_type]
        return provider_class(api_key, model_name, **kwargs)
    
    @classmethod
    def get_all_available_models(cls) -> Dict[ProviderType, List[ModelInfo]]:
        """è·å–æ‰€æœ‰æä¾›å•†çš„å¯ç”¨æ¨¡å‹"""
        models = {}
        for provider_type, provider_class in cls._providers.items():
            try:
                # åˆ›å»ºä¸´æ—¶å®ä¾‹æ¥è·å–æ¨¡å‹åˆ—è¡¨
                temp_provider = provider_class("dummy_key", "dummy_model")
                models[provider_type] = temp_provider.get_available_models()
            except Exception as e:
                logger.warning(f"æ— æ³•è·å–{provider_type.value}çš„æ¨¡å‹åˆ—è¡¨: {e}")
                models[provider_type] = []
        return models
